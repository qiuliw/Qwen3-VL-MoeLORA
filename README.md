# Qwen3-VL-MoeLoRA

![](image/image.png)

> åœ¨åƒé—®æœ€æ–°çš„å¤šæ¨¡æ€ image-text æ¨¡å‹ **Qwen3-VL-4B-Instruct** ä¸Šå®Œæˆ MOELoRAï¼ˆæ··åˆä¸“å®¶ LoRAï¼‰å¾®è°ƒï¼ŒåŒæ—¶æ‰“é€š COCO 2014 æ•°æ®å¤„ç†ã€SwanLab ç›‘æ§ã€LangChain + RAG + Qt å¤šæ™ºèƒ½ä½“æ¨¡å‹éƒ¨ç½²çš„å…¨è¿‡ç¨‹ã€‚
> é¦–å…ˆç®€å•ä»‹ç»ä¸€ä¸‹ Qwen-vl å’Œ Qwen3-vl

<details>
<summary><strong>Qwen-VL</strong></summary>

> [Qwen3-VL Technical Report](https://arxiv.org/pdf/2511.21631)

å¦‚ä¸‹å›¾ï¼ŒQwen-VL ç³»åˆ— çš„è®­ç»ƒè¿‡ç¨‹åˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼š

- **Stage1 ä¸ºé¢„è®­ç»ƒ**ï¼Œç›®æ ‡æ˜¯ä½¿ç”¨å¤§é‡çš„å›¾æ–‡ Pair å¯¹æ•°æ®å¯¹é½è§†è§‰æ¨¡å—å’Œ LLM çš„ç‰¹å¾ï¼Œè¿™ä¸ªé˜¶æ®µå†»ç»“ LLM æ¨¡å—çš„å‚æ•°ï¼›
- **Stage2 ä¸ºå¤šä»»åŠ¡é¢„è®­ç»ƒ**ï¼Œä½¿ç”¨æ›´é«˜è´¨é‡çš„å›¾æ–‡å¤šä»»åŠ¡æ•°æ®ï¼ˆä¸»è¦æ¥æºè‡ªå¼€æº VL ä»»åŠ¡ï¼Œéƒ¨åˆ†è‡ªå»ºæ•°æ®é›†ï¼‰ï¼Œæ›´é«˜çš„å›¾ç‰‡åƒç´ è¾“å…¥ï¼Œå…¨å‚æ•°è®­ç»ƒï¼›
- **Stage3 ä¸ºæŒ‡ä»¤å¾®è°ƒé˜¶æ®µ**ï¼Œè¿™ä¸ªé˜¶æ®µå†»ç»“è§†è§‰ Encoder æ¨¡å—ï¼Œä½¿ç”¨çš„æ•°æ®ä¸»è¦æ¥è‡ªå¤§æ¨¡å‹ Self-Instruction æ–¹å¼è‡ªåŠ¨ç”Ÿæˆï¼Œç›®æ ‡æ˜¯æå‡æ¨¡å‹çš„æŒ‡ä»¤éµå¾ªå’Œå¤šè½®å¯¹è¯èƒ½åŠ›ã€‚

![](image/Qwen-vl.png)
è€Œæœ€æ–°å¼€æºçš„ Qwen3-vl ä¸»è¦æœ‰å¦‚ä¸‹åˆ›æ–°ï¼š

- **Interleaved-MRoPEï¼š** åœ¨ æ—¶é—´/å®½/é«˜å¤šç»´åº¦åšå…¨é¢‘ç‡åˆ†é…çš„ä½ç½®ç¼–ç ï¼Œæå‡é•¿è§†é¢‘æ—¶åºæ¨ç†ã€‚
- **DeepStackï¼š** èåˆå¤šå±‚ ViT è§†è§‰ç‰¹å¾ï¼Œå¼ºåŒ–ç»†ç²’åº¦å¯¹é½ä¸è¯†åˆ«ã€‚
- **Textâ€“Timestamp Alignmentï¼š** ä» T-RoPE èµ°å‘â€œæ–‡æœ¬-æ—¶é—´æˆ³â€ç²¾å‡†å¯¹é½ï¼Œåˆ©äºäº‹ä»¶çº§è§†é¢‘å®šä½ã€‚
  ![](image/qwen3-vl.jpg)

</details>

---

## ğŸš€ é¡¹ç›®ä»‹ç»ä¸æ¨¡å—æ¦‚è§ˆ

1. **æ•°æ® â†’ è®­ç»ƒ â†’ æ¨ç†å…¨é“¾è·¯è„šæœ¬é½å…¨**ï¼š`download_data2csv.py` è´Ÿè´£æ‹‰å– & æ¸…æ´— COCO Captionï¼Œ`csv2json.py` é€‚é… Qwen3-VL æ ¼å¼ï¼Œ`MoeLORA.py/ lora.py` å¤„ç† LoRA / MoeLoRA è®­ç»ƒï¼Œ`test.py` å¿«é€ŸéªŒè¯ï¼Œ`multi_agent/` åˆ™æä¾› LangChain + RAG + Qt å¤šæ™ºèƒ½ä½“éƒ¨ç½²ã€‚
2. **æœ¬åœ°åŒ– RAG + å¤šæ¨¡æ€å¤šæ™ºèƒ½ä½“åŠ©æ‰‹**ï¼šPyQt5 æ¡Œé¢ç«¯ UIï¼Œæ•´åˆ LangChain æ£€ç´¢ã€FAISS å‘é‡åº“ä¸æœ¬åœ° Qwen3-VL æ¨ç†ï¼Œæ”¯æŒæ–‡æœ¬/å›¾åƒé—®ç­”ã€ä¸€é”®å¼€å…³çŸ¥è¯†åº“å¼•ç”¨ã€‚

- Advanced RAGï¼šBM25 + FAISS æ··åˆå¬å›ï¼Œç»“åˆ `BAAI/bge-reranker-base` Cross-Encoder é‡æ’åºï¼Œé»˜è®¤è¾“å‡ºæœ€ç›¸å…³çš„ Top-N ç‰‡æ®µã€‚
- Multi-Agent å‡çº§ï¼šæ–°å¢ Reviewer è‡ªæ£€æ­¥éª¤ï¼ˆæœ€å¤šé‡å†™ä¸€æ¬¡ç­”æ¡ˆï¼‰ä»¥åŠ MCP é£æ ¼ `save_session_summary` å·¥å…·ï¼Œå¯æŠŠèŠå¤©è®°å½•ä¸€é”®å¯¼å‡º Markdownã€‚

3. **è½»é‡æ˜¾å­˜å‹å¥½**ï¼šé»˜è®¤ 4-bit NF4 é‡åŒ– + LoRAï¼Œåªéœ€å•å¡ 8G ä¹Ÿèƒ½è·‘å®Œå¾®è°ƒæµç¨‹ã€‚
4. **SwanLab å…¨ç¨‹å¯è§†åŒ–**ï¼šè®­ç»ƒæ—¥å¿—ã€æŒ‡æ ‡å¯è§†åŒ–é½å¤‡ï¼Œä¾¿äºè°ƒä¼˜ä¸å¤ç°ã€‚

é¡¹ç›®æä¾› â€œæ•°æ®ä¸‹è½½ â†’ æ ¼å¼è½¬æ¢ â†’LoRA / MoeLoRA è®­ç»ƒ â†’ æœ¬åœ°æ¨ç† â†’ å¤šæ™ºèƒ½ä½“éƒ¨ç½²â€ çš„æœ€å°å¯å¤ç°å·¥ç¨‹ï¼Œå¸®åŠ©ä½ å¿«é€ŸéªŒè¯è‡ªå®šä¹‰çŸ¥è¯†åº“ + å¤šæ¨¡æ€é—®ç­”çš„å®Œæ•´é—­ç¯ã€‚

> ä»“åº“ä¸è‡ªå¸¦ Qwen3-VL-4B-Instruct æƒé‡ä¸ COCO æ•°æ®é›†ï¼Œè¯·æŒ‰ä¸‹æ–‡æŒ‡å¼•ä¸‹è½½åˆ°æŒ‡å®šç›®å½•ã€‚

## ğŸ“ é¡¹ç›®ç»“æ„

```text
Qwen3-VL-MoeLORA/
â”œâ”€â”€ coco_2014_caption/              # æ•°æ®é›†ä¸‹è½½ & è½¬æ¢äº§ç‰©
â”œâ”€â”€ multi_agent/                    # LangChain + RAG + Qt å¤šæ™ºèƒ½ä½“åŠ©æ‰‹
â”‚   â”œâ”€â”€ main_app.py                 # PyQt5 ä¸»ç¨‹åº
â”‚   â”œâ”€â”€ langchain_wrappers.py       # æœ¬åœ° Qwen æ–‡æœ¬/å¤šæ¨¡æ€å°è£…
â”‚   â”œâ”€â”€ multi_agent.py              # Planner / Manager / Knowledge / Reviewer / Responder ç¼–æ’
â”‚   â”œâ”€â”€ model_client.py             # æ¨¡å‹åŠ è½½ä¸å›¾ç‰‡è¾“å…¥å¤„ç†
â”‚   â”œâ”€â”€ rag_pipeline.py             # çŸ¥è¯†åº“åŠ è½½ + æ··åˆæ£€ç´¢ + äº¤å‰ç¼–ç é‡æ’åº
â”‚   â”œâ”€â”€ requirements.txt            # å­æ¨¡å—ä¾èµ–ï¼ˆPyQt5ã€LangChainã€FAISS ç­‰ï¼‰
â”‚   â”œâ”€â”€ tools.py                    # MCP é£æ ¼å·¥å…·ï¼ˆsave_session_summary ç­‰ï¼‰
â”‚   â””â”€â”€ knowledge_base/             # é»˜è®¤çŸ¥è¯†åº“ï¼Œå¯æ”¾ txt/md/pdf
â”œâ”€â”€ download_model.py               # ä¸‹è½½ Qwen3-VL åŸºåº§åˆ°æœ¬åœ°
â”œâ”€â”€ download_data2csv.py            # æ‹‰å– ModelScope æ•°æ®é›†
â”œâ”€â”€ csv2json.py                     # æ•°æ®è½¬æ¢è„šæœ¬
â”œâ”€â”€ MoeLORA.py / lora.py            # LoRA / MoeLoRA è®­ç»ƒ
â”œâ”€â”€ test.py                         # æœ¬åœ°æ¨ç† DEMO
â”œâ”€â”€ main_app_ui.py / main_langchain_ui.py # Qt UI ç¤ºä¾‹
â”œâ”€â”€ output/                         # LoRA è®­ç»ƒç»“æœ
â”œâ”€â”€ qwen3-vl-4b-instruct/           # é¢„æœŸæ”¾ç½®å®˜æ–¹åŸºåº§
â””â”€â”€ requirements.txt                # é¡¶å±‚ä¾èµ–
```

## âš™ï¸ å¿«é€Ÿä¸Šæ‰‹

### 0. ç¯å¢ƒè¦æ±‚ï¼šPyTorch å’Œ CUDA ç‰ˆæœ¬å¯¹åº”

**âš ï¸ é‡è¦æç¤ºï¼š** æœ¬é¡¹ç›®éœ€è¦ GPU æ”¯æŒï¼Œè¯·ç¡®ä¿å®‰è£…æ”¯æŒ CUDA çš„ PyTorch ç‰ˆæœ¬ï¼Œè€Œä¸æ˜¯ CPU ç‰ˆæœ¬ã€‚

#### æ£€æŸ¥å½“å‰ç¯å¢ƒ
```bash
# æ£€æŸ¥ GPU æ˜¯å¦å¯ç”¨
python -c "import torch; print('PyTorch version:', torch.__version__); print('CUDA available:', torch.cuda.is_available()); print('CUDA version:', torch.version.cuda if torch.cuda.is_available() else 'N/A')"

# æ£€æŸ¥ NVIDIA é©±åŠ¨å’Œ CUDA ç‰ˆæœ¬
nvidia-smi
```

#### PyTorch å’Œ CUDA ç‰ˆæœ¬å¯¹åº”è¡¨

| PyTorch ç‰ˆæœ¬ | CUDA ç‰ˆæœ¬ | å®‰è£…å‘½ä»¤ |
|-------------|----------|---------|
| 2.6.0 | CUDA 12.4 | `pip install torch torchvision --index-url https://download.pytorch.org/whl/cu124` |
| 2.5.0 | CUDA 12.4 | `pip install torch torchvision --index-url https://download.pytorch.org/whl/cu124` |
| 2.4.0 | CUDA 12.1 | `pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121` |
| 2.3.0 | CUDA 12.1 | `pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121` |
| 2.2.0 | CUDA 11.8 | `pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118` |
| 2.1.0 | CUDA 11.8 | `pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118` |

#### å®‰è£…æ­¥éª¤

1. **æ£€æŸ¥ NVIDIA é©±åŠ¨ç‰ˆæœ¬**ï¼ˆé€šè¿‡ `nvidia-smi` æŸ¥çœ‹ï¼‰
2. **æ ¹æ®é©±åŠ¨æ”¯æŒçš„ CUDA ç‰ˆæœ¬é€‰æ‹©å¯¹åº”çš„ PyTorch ç‰ˆæœ¬**
3. **å¸è½½ CPU ç‰ˆæœ¬çš„ PyTorch**ï¼ˆå¦‚æœå·²å®‰è£…ï¼‰ï¼š
   ```bash
   pip uninstall torch torchvision -y
   ```
4. **å®‰è£…æ”¯æŒ CUDA çš„ PyTorch**ï¼ˆä»¥ CUDA 12.4 ä¸ºä¾‹ï¼‰ï¼š
   ```bash
   pip install torch torchvision --index-url https://download.pytorch.org/whl/cu124
   ```
5. **éªŒè¯å®‰è£…**ï¼š
   ```bash
   python -c "import torch; assert torch.cuda.is_available(), 'CUDA not available!'; print('âœ… CUDA available:', torch.cuda.is_available()); print('GPU:', torch.cuda.get_device_name(0))"
   ```

> ğŸ’¡ **æç¤º**ï¼šå¦‚æœ `torch.cuda.is_available()` è¿”å› `False`ï¼Œè¯·æ£€æŸ¥ï¼š
> - NVIDIA é©±åŠ¨æ˜¯å¦æ­£ç¡®å®‰è£…
> - PyTorch æ˜¯å¦å®‰è£…äº† CPU ç‰ˆæœ¬ï¼ˆç‰ˆæœ¬å·ä¸­ä¸åŒ…å« `+cu`ï¼‰
> - CUDA ç‰ˆæœ¬æ˜¯å¦åŒ¹é…

### 1. å…‹éš† & åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ

ä½¿ç”¨ [uv](https://github.com/astral-sh/uv) å¿«é€Ÿåˆ›å»º Python 3.12 ç¯å¢ƒï¼š

**Windows:**
```powershell
# å®‰è£… uv (å¦‚æœæœªå®‰è£…)
powershell -ExecutionPolicy Bypass -Command "irm https://astral.sh/uv/install.ps1 | iex"

# å°† uv æ·»åŠ åˆ° PATH (å½“å‰ä¼šè¯)
$env:Path = "$env:USERPROFILE\.local\bin;$env:Path"

# æ–¹æ³•2: æ°¸ä¹…æ·»åŠ åˆ° PATH (éœ€è¦é‡å¯ PowerShell ç»ˆç«¯æ‰èƒ½ç”Ÿæ•ˆ)
# [Environment]::SetEnvironmentVariable("Path", "$env:USERPROFILE\.local\bin;$env:Path", "User")
# ç„¶åå…³é—­å¹¶é‡æ–°æ‰“å¼€ PowerShell ç»ˆç«¯

# éªŒè¯ uv æ˜¯å¦å¯ç”¨
uv --version
# å¦‚æœæ˜¾ç¤ºç‰ˆæœ¬å·ï¼Œè¯´æ˜é…ç½®æˆåŠŸ

# å…‹éš†é¡¹ç›®
git clone https://github.com/<your-account>/Qwen3-VL-MoeLORA.git
cd Qwen3-VL-MoeLORA

# ä½¿ç”¨ uv åˆ›å»º Python 3.12 è™šæ‹Ÿç¯å¢ƒå¹¶å®‰è£…ä¾èµ–
uv venv --python 3.12
.venv\Scripts\activate

# âš ï¸ é‡è¦ï¼šæ£€æŸ¥ PyTorch å’Œ CUDA ç‰ˆæœ¬
# ç¡®ä¿å®‰è£…æ”¯æŒ CUDA çš„ PyTorch ç‰ˆæœ¬ï¼Œè€Œä¸æ˜¯ CPU ç‰ˆæœ¬
python -c "import torch; print('PyTorch version:', torch.__version__); print('CUDA available:', torch.cuda.is_available())"
# å¦‚æœ CUDA available ä¸º Falseï¼Œè¯·å‚è€ƒä¸‹é¢çš„ç‰ˆæœ¬å¯¹åº”è¡¨é‡æ–°å®‰è£…
uv pip install -r requirements.txt
```

**Linux/Mac:**
```bash
# å®‰è£… uv (å¦‚æœæœªå®‰è£…)
curl -LsSf https://astral.sh/uv/install.sh | sh

# å…‹éš†é¡¹ç›®
git clone https://github.com/<your-account>/Qwen3-VL-MoeLORA.git
cd Qwen3-VL-MoeLORA

# ä½¿ç”¨ uv åˆ›å»º Python 3.12 è™šæ‹Ÿç¯å¢ƒå¹¶å®‰è£…ä¾èµ–
uv venv --python 3.12
source .venv/bin/activate
uv pip install -r requirements.txt
```

> å¤šæ™ºèƒ½ä½“æ¡Œé¢ç«¯ä¾èµ–é¢å¤–çš„ PyQt5/FAISSï¼Œå¯åœ¨ `multi_agent/` ç›®å½•æ‰§è¡Œ `uv pip install -r requirements.txt`ã€‚

### 2. ä¸‹è½½æ¨¡å‹ä¸æ•°æ®é›†

```powershell
# æ‹‰å–å®˜æ–¹åŸºåº§
python download_model.py --target ./qwen3-vl-4b-instruct

# ä¸‹è½½ COCO Caption ç¤ºä¾‹å¹¶å†™å…¥ CSV
python download_data2csv.py --output ./coco_2014_caption/train.csv

# è½¬æ¢ä¸º Qwen3-VL JSONï¼ˆå¯æŒ‡å®šæ¡ç›®ï¼‰
python csv2json.py --csv ./coco_2014_caption/train.csv --json ./coco_2014_caption/train.json --top_k 500
```

#### ğŸ“ è‡ªå®šä¹‰å‚æ•°è¯´æ˜

| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ | ä¿®æ”¹ä½ç½® |
|------|------|--------|----------|
| **ModelScope Token** | ç”¨äºä¸‹è½½ ModelScope æ•°æ®é›†å’Œæ¨¡å‹çš„è®¤è¯ tokenã€‚å¦‚æœä¸‹è½½å¤±è´¥ï¼Œå¯åœ¨ [ModelScope](https://modelscope.cn) æ³¨å†Œè´¦å·å¹¶è·å– tokenï¼Œç„¶åè®¾ç½®ç¯å¢ƒå˜é‡ï¼š<br>`$env:MODELSCOPE_API_TOKEN="your_token"` (Windows)<br>`export MODELSCOPE_API_TOKEN="your_token"` (Linux/Mac) | æ—  | ç¯å¢ƒå˜é‡æˆ– `download_data2csv.py` / `download_model.py` ä¸­çš„ `token` å‚æ•° |
| **SwanLab Token** | ç”¨äºè®­ç»ƒæ•°æ®å¯è§†åŒ–çš„è®¤è¯ tokenã€‚SwanLab æä¾›è®­ç»ƒè¿‡ç¨‹çš„å¯è§†åŒ–ç›‘æ§ï¼ŒåŒ…æ‹¬æŸå¤±æ›²çº¿ã€å­¦ä¹ ç‡ç­‰æŒ‡æ ‡ã€‚è·å–æ–¹å¼ï¼š<br>1. è®¿é—® [SwanLab](https://swanlab.cn) æ³¨å†Œè´¦å·<br>2. åœ¨ä¸ªäººè®¾ç½®ä¸­è·å– API Key<br>3. åœ¨ `config.yaml` ä¸­è®¾ç½® `swanlab.api_key: "your_api_key"` | æ—  | `config.yaml` é…ç½®æ–‡ä»¶ä¸­çš„ `swanlab.api_key` |

### 3. å¿«é€Ÿæ¨ç†ï¼ˆåŸºåº§æˆ– LoRAï¼‰

```powershell
python test.py --model ./qwen3-vl-4b-instruct --image ./image/demo.jpg --prompt "æè¿°è¿™å¼ å›¾ç‰‡"
```

è‹¥å·²å®Œæˆ LoRA è®­ç»ƒï¼Œå¯å°† `--model` æŒ‡å‘åˆå¹¶åçš„æƒé‡æˆ–ç›´æ¥åœ¨ `test.py` ä¸­åŠ è½½ `PeftModel`ã€‚

### 4. å¯åŠ¨ LangChain + RAG + Qt å¤šæ™ºèƒ½ä½“åŠ©æ‰‹

```powershell
cd multi_agent
pip install -r requirements.txt  # é¦–æ¬¡æ‰§è¡Œ
python main_app.py
```

å·¦ä¾§è¾“å…¥é—®é¢˜/ä¸Šä¼ å›¾ç‰‡ï¼Œå³ä¾§ä¼šå±•ç¤º Planner/Manager è®¡åˆ’ã€RAG æ£€ç´¢æ‘˜è¦ä¸æœ€ç»ˆå›å¤ã€‚

#### ğŸŒŸ æ–°ç‰ˆå¤šæ™ºèƒ½ä½“äº®ç‚¹

- **Advanced RAG**ï¼šBM25 + FAISS èåˆå¬å›ï¼Œå åŠ  Cross-Encoder é‡æ’åºï¼Œæ˜¾è‘—é™ä½â€œæŸ¥ä¸åˆ°/æŸ¥ä¸å‡†â€é—®é¢˜ã€‚
- **Reviewer QA Loop**ï¼šResponder ç”Ÿæˆç­”æ¡ˆåï¼ŒReviewer ä»¥ PASS/RETRY å½¢å¼å¤æ ¸ï¼›å¦‚ä¸åˆæ ¼ï¼Œä¼šæŠŠå»ºè®®åé¦ˆç»™ Responder é‡å†™ä¸€æ¬¡ï¼Œæå‡å›ç­”å®Œæ•´æ€§ã€‚
- **MCP é£æ ¼å½’æ¡£å·¥å…·**ï¼šå†…ç½® `save_session_summary`ï¼Œå½“èŠå¤©è¯­å¥åŒ…å«â€œæ€»ç»“å¯¹è¯ / ä¿å­˜è®°å½• / archive / summary â€¦â€æ—¶ï¼Œä¼šæŠŠå®Œæ•´å¯¹è¯ã€è°ƒåº¦è®¡åˆ’ä¸æ£€ç´¢è¯æ®å†™å…¥ `multi_agent/output/reports/*.md`ï¼Œå›ç­”é‡Œä¹Ÿä¼šæ˜¾ç¤ºæ–‡ä»¶è·¯å¾„ã€‚
- **GUI è‡ªåŠ¨è®°å½•ä¸Šä¸‹æ–‡**ï¼šå‰ç«¯æŒç»­è·Ÿè¸ªæ¯è½®é—®ç­”ï¼Œç”Ÿæˆæ€»ç»“æ—¶æ— éœ€æ‰‹å·¥å¤åˆ¶å†…å®¹ï¼ŒAgent ä¼šè‡ªåŠ¨è¯»å–å†å²å¯¹è¯ã€‚

> å¯ä»¥çœ‹åˆ°æ™ºèƒ½ä½“æ­£ç¡®å›ç­”æˆ‘çš„é—®é¢˜ï¼šå°è‹”è—“äºä»Šå¹´ 9 æœˆ 25 æ—¥ä¿é€åˆ°å¦é—¨å¤§å­¦ï¼ˆä¹Ÿå°±æ˜¯æœ¬äººï¼Œå“ˆå“ˆï¼Œç›®å‰ GUI è¿˜æœ‰ç‚¹å°é—®é¢˜ï¼Œæ–‡å­—å†…å®¹ä¸é•¿å±•ç¤ºä¸å…¨ï¼‰

![](image/image1.png)

> å¯¼å‡ºä¼šè¯è®°å½•
> ![](image/images3.png)

### 5. LoRA / MoeLoRA å¾®è°ƒ

é¡¹ç›®æ”¯æŒä¸¤ç§é…ç½®æ–¹å¼ï¼š**YAML é…ç½®æ–‡ä»¶**ï¼ˆæ¨èï¼‰å’Œ**å‘½ä»¤è¡Œå‚æ•°**ï¼ˆå¿«é€Ÿè¦†ç›–ï¼‰ã€‚

#### æ–¹å¼ä¸€ï¼šä½¿ç”¨é…ç½®æ–‡ä»¶ï¼ˆæ¨èï¼‰

```powershell
# ä½¿ç”¨é»˜è®¤é…ç½®æ–‡ä»¶ config.yaml
python MoeLORA.py

# ä½¿ç”¨è‡ªå®šä¹‰é…ç½®æ–‡ä»¶
python MoeLORA.py --config my_config.yaml
```

æ‰€æœ‰å‚æ•°åœ¨ `config.yaml` ä¸­ç»Ÿä¸€ç®¡ç†ï¼Œå‚è€ƒ LLaMAFactory é£æ ¼ã€‚è¯¦ç»†é…ç½®è¯´æ˜è§ [README_CONFIG.md](README_CONFIG.md)ã€‚

#### æ–¹å¼äºŒï¼šå‘½ä»¤è¡Œå‚æ•°è¦†ç›–

```powershell
# ä½¿ç”¨é…ç½®æ–‡ä»¶ï¼Œå¹¶é€šè¿‡å‘½ä»¤è¡Œå‚æ•°è¦†ç›–éƒ¨åˆ†é…ç½®
python MoeLORA.py \
  --model ./qwen3-vl-4b-instruct \
  --train_json ./coco_2014_caption/train.json \
  --output_dir ./output/Qwen3-VL-4Blora
```

**å‘½ä»¤è¡Œå‚æ•°ä¼˜å…ˆçº§é«˜äºé…ç½®æ–‡ä»¶**ï¼Œå¯ä»¥å¿«é€Ÿè¦†ç›–å¸¸ç”¨å‚æ•°è€Œæ— éœ€ä¿®æ”¹é…ç½®æ–‡ä»¶ã€‚

**æ”¯æŒçš„å‘½ä»¤è¡Œå‚æ•°**ï¼ˆè¦†ç›–åŒåé…ç½®ï¼Œæœªåˆ—å‡ºçš„å‚æ•°ä»ä» `config.yaml` è¯»å–ï¼‰ï¼š
- `--config`ï¼šé…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ `config.yaml`ï¼‰
- `--model`ï¼šæ¨¡å‹è·¯å¾„ï¼ˆè¦†ç›– `model.model_name_or_path`ï¼‰
- `--train_json`ï¼šè®­ç»ƒæ•°æ® JSON è·¯å¾„ï¼ˆè¦†ç›– `dataset.train_json_path`ï¼‰
- `--output_dir`ï¼šè¾“å‡ºç›®å½•ï¼ˆè¦†ç›– `training.output_dir`ï¼‰
- è®­ç»ƒè¶…å‚è¦†ç›–ï¼š`--per_device_train_batch_size`ã€`--gradient_accumulation_steps`ã€`--num_train_epochs`ã€`--save_steps`ã€`--logging_steps`ã€`--logging_first_step`ã€`--learning_rate`ã€`--fp16`

é»˜è®¤è®­ç»ƒè¶…å‚ï¼ˆä¸åŸè„šæœ¬ä¸€è‡´ï¼Œè¯¦è§ `config.yaml`ï¼‰ï¼š
- `per_device_train_batch_size=1`ï¼Œ`gradient_accumulation_steps=8`
- `num_train_epochs=5`ï¼ˆæœªä½¿ç”¨ `max_steps`ï¼‰
- `save_steps=100`ï¼Œ`logging_steps=10`ï¼Œ`logging_first_step=5`
- `learning_rate=1e-4`
- `fp16=true`ï¼Œ`gradient_checkpointing=true`

è„šæœ¬é»˜è®¤å¯ç”¨ BitsAndBytes 4-bit ä¸ PEFTï¼Œå¯æ ¹æ®æ˜¾å­˜æƒ…å†µè°ƒæ•´ `r`ã€`lora_alpha`ã€`gradient_accumulation_steps` ç­‰å‚æ•°ã€‚è®­ç»ƒå®Œæˆåäº§ç‰©ä½äº `output/`ï¼Œå¯è¢«å¤šæ™ºèƒ½ä½“æˆ– `test.py` ç›´æ¥åŠ è½½ã€‚

##### ä»£ç åŒ…å«å®Œæ•´æµç¨‹ä»£ç 

ï¼ˆæ³¨ï¼šä¸åŒ…å« Qwen3-VL-4B-Instruct æ¨¡å‹ä»£ç å’Œæƒé‡ï¼Œè¯·è‡ªè¡Œä¸‹è½½ï¼‰ï¼š
coco_2014_caption æ•°æ®é›† [coco_2014_caption](https://modelscope.cn/datasets/modelscope/coco_2014_caption/quickstart)
Qwen3-VL-4B-Instruct æ¨¡å‹ [Qwen3-VL-4B-Instruct](https://huggingface.co/Qwen/Qwen3-VL-4B-Instruct)
ä¸‹è½½åç›´æ¥æ”¾åœ¨é¡¹ç›®æ‰€åœ¨ç›®å½•å³å¯ï¼ˆ`./qwen3-vl-4b-instruct`ï¼‰

ModelScope æ•°æ®é›†åŠ è½½->å¤šæ¨¡æ€æ•°æ®é¢„å¤„ç†->lora\MOELora å¾®è°ƒé…ç½®->SwanLab è®­ç»ƒå¯è§†åŒ–åŠå¾®è°ƒåæ¨¡å‹æ¨ç†

## ğŸ§  LangChain + RAG + Qt å¤šæ™ºèƒ½ä½“åŠ©æ‰‹

- **Planner Agent**ï¼šæ‹†è§£ä»»åŠ¡ï¼Œåˆ—å‡º 3-5 ä¸ªå…³é”®æ­¥éª¤å¹¶æç¤ºæ˜¯å¦éœ€è¦çŸ¥è¯†åº“ã€‚
- **Manager Agent**ï¼šç»“åˆ Planner è¾“å‡ºä¸ RAG é¢„è§ˆï¼Œå†³å®šæ˜¯å¦ç»§ç»­æ£€ç´¢ã€ç»™å‡ºé¢å¤–æç¤ºã€‚
- **Knowledge Agent**ï¼šåŸºäº `rag_pipeline.py` æ„å»ºçš„ BM25 + FAISS æ··åˆæ£€ç´¢ + Cross-Encoder é‡æ’åºï¼Œæ”¯æŒ `.txt/.md/.pdf` å¤šç¼–ç åŠ è½½ã€‚
- **Responder Agent**ï¼šé€šè¿‡ `model_client.py` è°ƒç”¨æœ¬åœ° Qwen3-VLï¼ˆæ”¯æŒ LoRA æƒé‡ & å›¾æ–‡è¾“å…¥ï¼‰ï¼Œå¹¶èƒ½æ¥æ”¶ Reviewer çš„æ”¹å†™å»ºè®®ã€‚
- **Reviewer Agent**ï¼šæ£€æŸ¥ç­”æ¡ˆæ˜¯å¦è§£å†³ç”¨æˆ·é—®é¢˜ï¼Œå¿…è¦æ—¶è¦æ±‚ Responder é‡ç­”ã€‚
- **Qt å‰ç«¯**ï¼š`main_app.py` å¤åˆ» `main_langchain_ui.py` çš„äº¤äº’ä½“éªŒï¼Œå±•ç¤º Planner/Manager é¢æ¿ã€RAG å‚è€ƒä¸æœ€ç»ˆç­”å¤

> é»˜è®¤çŸ¥è¯†åº“ç›®å½•ï¼š`multi_agent/knowledge_base/`ï¼Œç•Œé¢å³ä¸‹è§’å¯å‹¾é€‰ â€œå¯ç”¨çŸ¥è¯†åº“æ£€ç´¢ (RAG)â€ å¼€å…³ã€‚è‹¥åªéœ€å½’æ¡£èŠå¤©ï¼Œè¯·è¾“å…¥â€œæ€»ç»“å¯¹è¯/ä¿å­˜è®°å½•â€ç­‰æŒ‡ä»¤ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨è°ƒç”¨ MCP å·¥å…·è¾“å‡º Markdownã€‚

##### ä¸Šä¼ æœ¬åœ°è®ºæ–‡åˆ°çŸ¥è¯†åº“ï¼Œå¯¹è®ºæ–‡è¿›è¡Œé—®ç­”

![](image/image0.png)

#### 1.æœ¬åœ°éƒ¨ç½²æ¨ç†

æ¨¡å‹ä» huggingface ä¸‹è½½åˆ°æœ¬åœ°åï¼Œå°† test.py ä¸­çš„ model_id æ¢ä¸ºæœ¬åœ°è·¯å¾„ï¼Œè¿è¡Œ test.py æ–‡ä»¶

![](image/image-20251018215438537.png)

#### 2.å¾®è°ƒ

lora é…ç½®ï¼Œè§ MoeLORA.py æ–‡ä»¶

```python
config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    # æ–°å¢è§†è§‰ç¼–ç å™¨å’Œäº¤å‰æ³¨æ„åŠ›å±‚ï¼ˆQwen3-VLç‰¹æœ‰æ¨¡å—ï¼‰
    target_modules=[
        # æ–‡æœ¬æ¨¡å—
        "q_proj", "k_proj", "v_proj", "o_proj"
        # è§†è§‰æ¨¡å—
        "visual_q_proj", "visual_k_proj"],
    inference_mode=False,
    r=8,  # 8Gæ˜¾å­˜å»ºè®®r=16ï¼ˆåŸ64å¯èƒ½æ˜¾å­˜ä¸è¶³ï¼‰
    lora_alpha=16,
    lora_dropout=0.05,
    bias="none",
)
```

![](image/image-20251019172207276.png)

å¾®è°ƒå›¾åƒ

![](image/image-20251019210718609.png)

<details>
<summary><strong>è®­ç»ƒè¿è¡Œï¼ˆlogï¼‰</strong></summary>

| æŒ‡æ ‡                          | LoRA                                                        | MoeLoRA                                                    |
| ----------------------------- | ----------------------------------------------------------- | ---------------------------------------------------------- |
| æ¨¡å‹ï¼ˆbaseï¼‰                  | `qwen3-vl-4b-instruct`ï¼ˆæœ¬åœ°è·¯å¾„ `./qwen3-vl-4b-instruct`ï¼‰ | `qwen3-vl-4b-instruct`ï¼ˆ`./qwen3-vl-4b-instruct`ï¼‰         |
| æ•°æ®é›†æ ·æœ¬æ•°ï¼ˆtrainï¼‰         | 496 examples                                                | 496 examples                                               |
| æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆè„šæœ¬ï¼‰        | 8192 tokens                                                 | 8192 tokens                                                |
| å¾®è°ƒæ–¹æ³•                      | LoRA (PEFT) + 4-bit quantization (bnb nf4)                  | MoeLoRA (å¤šä¸“å®¶ LoRA) + 4-bit quantization (bnb nf4)       |
| æ³¨å…¥çš„å¯è®­ç»ƒå‚æ•°ï¼ˆæ—¥å¿—ï¼‰      | 5,898,240 trainable params                                  | 10,298,240 trainable params                                |
| æ¨¡å‹æ€»å‚æ•°é‡ï¼ˆæ—¥å¿—ï¼‰          | 4,443,714,048 å…¨é‡å‚æ•°                                      | 4,443,714,048 params                                       |
| trainable ç™¾åˆ†æ¯”ï¼ˆæ—¥å¿—ï¼‰      | ~0.1327%                                                    | â‰ˆ 0.245%                                                   |
| è®­ç»ƒè½®æ¬¡ (epochs)             | 5.0 epochs                                                  | 5.0 epochs                                                 |
| æ€»è®­ç»ƒæ­¥æ•°ï¼ˆglobal stepsï¼‰    | 310 steps                                                   | 310 steps                                                  |
| æ¯ epoch æ­¥æ•°                 | ~62 steps/epoch                                             | 62                                                         |
| per_device_train_batch_size   | 1 (å·²åœ¨è„šæœ¬ä¼˜åŒ–ä¸º 1)                                        | 1                                                          |
| gradient_accumulation_steps   | 8                                                           | 8                                                          |
| å­¦ä¹ ç‡ï¼ˆåˆå§‹ï¼‰                | 1e-4                                                        | 1e-4                                                       |
| å­¦ä¹ ç‡ï¼ˆè®­ç»ƒæœ«æœŸï¼‰            | â‰ˆ3.23e-07                                                   | è®­ç»ƒæ—¥å¿—æ˜¾ç¤ºæœ€å step çš„ lr: `3.2258e-07`ï¼ˆçº¿æ€§/è°ƒåº¦è¡°å‡ï¼‰ |
| è®­ç»ƒæ€»æ—¶é•¿                    | 5108.6867 s â‰ˆ 85.15 min                                     | 6595.6952â‰ˆ109min                                           |
| å¹³å‡ train_lossï¼ˆå…¨ç¨‹ï¼‰       | ~1.70645                                                    | ~1.65432                                                   |
| åˆå§‹ batch lossï¼ˆç¬¬ä¸€æ¡æ—¥å¿—ï¼‰ | 4.8942                                                      | 4.7856                                                     |
| è®­ç»ƒæ ·æœ¬åå                  | 0.485 samples/s                                             | 0.44 samples/s                                             |
| è®­ç»ƒæ­¥åå                    | 0.061 steps/s                                               | 0.055 steps/s                                              |
| æ¢¯åº¦èŒƒæ•°ï¼ˆè§‚æµ‹èŒƒå›´ï¼‰          | çº¦ 1.25 â€” 3.75ï¼ˆè§‚æµ‹ï¼‰                                      | 1.5 â€” 4.5(æ³¢åŠ¨)                                            |
| é‡åŒ–æ–¹å¼                      | 4-bit NF4 åŒé‡åŒ–ï¼Œcompute_dtype=float16                     | 4-bit NF4 åŒé‡åŒ–ï¼Œcompute_dtype=float16                    |
| mixed-precision               | fp16=Trueï¼ˆTrainerï¼‰                                        | `fp16=True`                                                |
| checkpoint ä¿¡æ¯               | ä¿å­˜åˆ° `./output/Qwen3-VL-4Blora`                           | ./output/Qwen3-VL-4Bmoelora/checkpoint-\*                  |
| è®­ç»ƒä¸­å·²è®°å½•ï¼ˆç›‘æ§ï¼‰          | SwanLabï¼ˆlogs/å¯è§†åŒ–ï¼‰                                      | SwanLabï¼ˆlogs/å¯è§†åŒ–ï¼‰                                     |

åŠ è½½è®­ç»ƒå¥½çš„ LoRA checkpoint åšæ¨ç†

```python
from peft import PeftModel
from transformers import AutoModelForImageTextToText

base = AutoModelForImageTextToText.from_pretrained(model_id,
                                                  quantization_config=bnb_config,
                                                  device_map={"": "cuda"},
                                                  trust_remote_code=True)
base.config.use_cache = False
infer_model = PeftModel.from_pretrained(base, "./output/Qwen3-VL-4Blora")  # æœ¬åœ°è·¯å¾„
infer_model.to("cuda").eval()

```

</details>

---

æ³¨æ„ï¼šä¸è¦æŠŠæœ¬åœ°è·¯å¾„ä»¥ `model_id=` å½¢å¼ä¼ ç»™ `from_pretrained` é‡Œä¼šè§¦å‘ HF repo id éªŒè¯ï¼ˆæ—¥å¿—é‡Œå·²è§é”™è¯¯æç¤ºï¼‰ã€‚ç›´æ¥æŠŠæœ¬åœ° checkpoint ç›®å½•è·¯å¾„ä½œä¸ºç¬¬ä¸€ä¸ªå‚æ•°ä¼ å…¥ `PeftModel.from_pretrained` å³å¯ã€‚

å¾®è°ƒåæ¨ç†ç»“æœ
![](image/PixPin_2025-11-03_17-12-57.png)

## ğŸ§­ é¡¹ç›®è§„åˆ’ï¼ˆRoadmapï¼‰

- [ ] **MCP (Model Context Protocol)**ï¼šå°†å¤šæ™ºèƒ½ä½“æ¨ç†å°è£…æˆ MCP Serverï¼Œæ–¹ä¾¿ IDE / Copilot Chat ç›´æ¥è°ƒç”¨ã€‚
- [ ] **çŸ¥è¯†åº“çƒ­æ›´æ–°**ï¼šæ”¯æŒè¿œç¨‹å‘é‡æ•°æ®åº“ï¼ˆMilvus/Elasticsearchï¼‰ä¸åœ¨çº¿æ–‡æ¡£è‡ªåŠ¨åŒæ­¥ã€‚
- [ ] **è¯„æµ‹è‡ªåŠ¨åŒ–**ï¼šå¼•å…¥ LLM-as-a-judgeã€BLEU/ROUGE ç­‰æŒ‡æ ‡ï¼Œå¯¹å›¾æ–‡å›ç­”åšè‡ªåŠ¨éªŒæ”¶ã€‚
- [ ] **æ•°æ®æ‰©å±•æµæ°´çº¿**ï¼šæŠ“å–è¡Œä¸šæŠ¥å‘Š â†’`csv2json.py` è‡ªåŠ¨è½¬æ¢ â†’ ä¸€é”®åŠ å…¥è®­ç»ƒæˆ– RAGã€‚

###### è‡´è°¢:

[Qwen3-VL](https://github.com/QwenLM/Qwen3-VL)
